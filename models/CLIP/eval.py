import torch
from tqdm import tqdm

def evaluate(model, dataloader, device):
    """
    æ›´æ ‡å‡†ã€æ›´å…¨é¢åœ°è¯„ä¼° CLIP æ¨¡å‹åœ¨â€œä¸€å¯¹å¤šâ€æ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
    åˆ†åˆ«è®¡ç®— Image-to-Text å’Œ Text-to-Image çš„ Recall@1 å’Œ Recall@5ã€‚
    """
    model.eval()
    
    # åˆå§‹åŒ–å„ç§æŒ‡æ ‡çš„è®¡æ•°å™¨
    total_samples = 0
    i2t_r1_correct = 0
    i2t_r5_correct = 0
    t2i_r1_correct = 0
    t2i_r5_correct = 0

    print("ğŸš€ Starting comprehensive evaluation for retrieval...")
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            # pixel_values: [N, C, H, W]
            # input_ids: [5*N, max_len]
            pixel_values = batch['pixel_values'].to(device)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            num_images = pixel_values.shape[0]
            num_texts = input_ids.shape[0]

            # 1. è·å–ç‰¹å¾
            image_features, text_features = model(pixel_values, input_ids, attention_mask)
            logit_scale = model.logit_scale.exp()

            # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            # logits_per_image (I2T): [N, 5*N]
            logits_per_image = image_features @ text_features.T * logit_scale
            # logits_per_text (T2I): [5*N, N]
            logits_per_text = logits_per_image.T
            
            # --- 3. Image-to-Text (I2T) Recall è®¡ç®— ---
            # å¯¹äºç¬¬ i å¼ å›¾ç‰‡ï¼Œæ­£ç¡®çš„æ–‡æœ¬ç´¢å¼•æ˜¯ [i*5, i*5+1, ..., i*5+4]
            
            # I2T Recall@1
            # æ‰¾åˆ°æ¯å¼ å›¾ç‰‡æœ€åŒ¹é…çš„æ–‡æœ¬ç´¢å¼•
            i2t_preds_r1 = logits_per_image.argmax(dim=1)
            # æ£€æŸ¥é¢„æµ‹æ˜¯å¦åœ¨æ­£ç¡®èŒƒå›´å†…
            for i in range(num_images):
                if (i * 5) <= i2t_preds_r1[i] < ((i + 1) * 5):
                    i2t_r1_correct += 1

            # I2T Recall@5
            # æ‰¾åˆ°æ¯å¼ å›¾ç‰‡æœ€åŒ¹é…çš„å‰5ä¸ªæ–‡æœ¬ç´¢å¼•
            _, i2t_preds_r5_indices = logits_per_image.topk(5, dim=1)
            # æ£€æŸ¥è¿™top-5çš„é¢„æµ‹ä¸­ï¼Œæ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ªè½åœ¨æ­£ç¡®çš„5ä¸ªç­”æ¡ˆé‡Œ
            for i in range(num_images):
                pred_indices = set(i2t_preds_r5_indices[i].tolist())
                true_indices = set(range(i * 5, (i + 1) * 5))
                if len(pred_indices & true_indices) > 0:
                    i2t_r5_correct += 1

            # --- 4. Text-to-Image (T2I) Recall è®¡ç®— ---
            # å¯¹äºç¬¬ j ä¸ªæ–‡æœ¬ï¼Œæ­£ç¡®çš„å›¾ç‰‡ç´¢å¼•æ˜¯ floor(j / 5)
            ground_truth = torch.arange(num_texts, device=device) // 5 # [0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,...]

            # T2I Recall@1
            t2i_preds_r1 = logits_per_text.argmax(dim=1)
            t2i_r1_correct += (t2i_preds_r1 == ground_truth).sum().item()

            # T2I Recall@5
            _, t2i_preds_r5_indices = logits_per_text.topk(5, dim=1) # [N*5, 5]
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦å‡ºç°åœ¨ top-5 é¢„æµ‹ä¸­
            t2i_r5_correct += (t2i_preds_r5_indices == ground_truth.unsqueeze(1)).any(dim=1).sum().item()
            
            total_samples += num_images

    # --- 5. è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆç»“æœ ---
    i2t_r1 = 100 * i2t_r1_correct / total_samples
    i2t_r5 = 100 * i2t_r5_correct / total_samples
    # å¯¹äºT2Iï¼Œæ ·æœ¬æ€»æ•°æ˜¯ 5 * total_samples
    t2i_r1 = 100 * t2i_r1_correct / (total_samples * 5)
    t2i_r5 = 100 * t2i_r5_correct / (total_samples * 5)

    print("\nâœ… Evaluation Results:")
    print(f"  Image-to-Text Recall@1: {i2t_r1:.2f}%")
    print(f"  Image-to-Text Recall@5: {i2t_r5:.2f}%")
    print("-" * 30)
    print(f"  Text-to-Image Recall@1: {t2i_r1:.2f}%")
    print(f"  Text-to-Image Recall@5: {t2i_r5:.2f}%")
    
    return {
        "i2t_r1": i2t_r1, "i2t_r5": i2t_r5,
        "t2i_r1": t2i_r1, "t2i_r5": t2i_r5
    }