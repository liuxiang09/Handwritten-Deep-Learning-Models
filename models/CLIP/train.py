import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import CLIPProcessor
from PIL import Image
import os
import random
from tqdm import tqdm
import argparse
import sys
from utils.flickr30k import Flickr30kDataset, collate_fn

# å¯¼å…¥ä½ çš„è‡ªå®šä¹‰æ¨¡å‹
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model.clip import CLIP
from model.vision_encoder import VisionEncoder, VisionEncoderPretrained
from model.text_encoder import TextEncoder
from model.modified_resnet import ModifiedResNet

# å¯¹æ¯”æŸå¤±å‡½æ•°
# ç”±äºä¸€ä¸ªå›¾ç‰‡æœ‰5ä¸ªæ–‡æœ¬ï¼Œæ‰€ä»¥æ— æ³•é‡‡ç”¨å¸¸è§„çš„äº¤å‰ç†µæŸå¤±å‡½æ•°
# class ContrastiveLoss(nn.Module):
#     def __init__(self):
#         super().__init__()

#     def forward(self, image_features, text_features, logit_scale):
#         # å½’ä¸€åŒ–ç‰¹å¾ (å·²ç»åœ¨ CLIP æ¨¡å‹çš„ forward ä¸­å®Œæˆ)
        
#         # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
#         logits = (image_features @ text_features.T) * logit_scale.exp()

#         # åˆ›å»ºæ ‡ç­¾ (å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬)
#         labels = torch.arange(len(logits)).to(logits.device)

#         # è®¡ç®—å›¾åƒåˆ°æ–‡æœ¬çš„æŸå¤± (è¡Œæ˜¯å›¾åƒï¼Œåˆ—æ˜¯æ–‡æœ¬)
#         loss_i = F.cross_entropy(logits, labels)
        
#         # è®¡ç®—æ–‡æœ¬åˆ°å›¾åƒçš„æŸå¤± (è½¬ç½® logitsï¼Œè¡Œæ˜¯æ–‡æœ¬ï¼Œåˆ—æ˜¯å›¾åƒ)
#         loss_t = F.cross_entropy(logits.T, labels)
        
#         # è¿”å›å¹³å‡æŸå¤±
#         return (loss_i + loss_t) / 2

class ContrastiveLoss(nn.Module):
    """
    èƒ½å¤Ÿæ­£ç¡®å¤„ç†ä¸€ä¸ªå›¾åƒå¯¹åº”å¤šä¸ªæ–‡æœ¬æè¿°çš„å¯¹æ¯”æŸå¤±å‡½æ•°ã€‚
    """
    def __init__(self):
        super().__init__()

    def forward(self, image_features, text_features, logit_scale):
        """
        Args:
            image_features: shape [N, D], N æ˜¯æ‰¹æ¬¡ä¸­çš„å›¾ç‰‡æ•°é‡ã€‚
            text_features: shape [5*N, D], å¯¹åº” N å¼ å›¾ç‰‡çš„ 5*N ä¸ªæ–‡æœ¬æè¿°ã€‚
            logit_scale: å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°ã€‚
        """
        device = image_features.device
        num_images = image_features.shape[0]
        num_texts = text_features.shape[0]

        # éªŒè¯è¾“å…¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
        if num_texts % num_images != 0 or num_texts // num_images != 5:
            raise ValueError("æ–‡æœ¬ç‰¹å¾æ•°é‡å¿…é¡»æ˜¯å›¾ç‰‡ç‰¹å¾æ•°é‡çš„5å€ã€‚")

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # logits_per_image shape: [N, 5*N]
        logits_per_image = (logit_scale.exp() * image_features @ text_features.T)
        # logits_per_text shape: [5*N, N]
        logits_per_text = logits_per_image.T

        # --- æ­£ç¡®çš„æŸå¤±è®¡ç®— ---

        # 1. è®¡ç®— loss_t (æ–‡æ‰¾å›¾): è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„å¤šç±»åˆ«åˆ†ç±»é—®é¢˜
        # æ¯ä¸ªæ–‡æœ¬éƒ½æœ‰ä¸€ä¸ªæ­£ç¡®çš„å›¾ç‰‡ç›®æ ‡ã€‚
        # åˆ›å»ºæ ‡ç­¾ [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ..., N-1, ...]
        text_labels = torch.arange(num_images, device=device).repeat_interleave(5)
        loss_t = F.cross_entropy(logits_per_text, text_labels)

        # 2. è®¡ç®— loss_i (å›¾æ‰¾æ–‡): è¿™æ˜¯ä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜
        # æ¯å¼ å›¾ç‰‡æœ‰5ä¸ªæ­£ç¡®çš„æ–‡æœ¬ç›®æ ‡ã€‚æ ‡å‡†çš„ cross_entropy ä¸é€‚ç”¨ã€‚
        # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª "å¤šçƒ­" (multi-hot) çš„æ ‡ç­¾çŸ©é˜µã€‚
        # ground_truth shape: [N, 5*N]
        ground_truth = torch.zeros(logits_per_image.shape, dtype=torch.float, device=device)
        for i in range(num_images):
            # å°†å›¾ç‰‡iå¯¹åº”çš„5ä¸ªæ–‡æœ¬ä½ç½®æ ‡è®°ä¸º1
            start_idx = i * 5
            end_idx = start_idx + 5
            ground_truth[i, start_idx:end_idx] = 1.0
        
        # ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤± (Binary Cross Entropy)
        # å®ƒå°†æ¯ä¸ªè¾“å‡ºlogitè§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„äºŒå…ƒåˆ†ç±»ï¼ˆæ˜¯/ä¸æ˜¯ æ­£ç¡®çš„åŒ¹é…ï¼‰
        loss_i = F.binary_cross_entropy_with_logits(logits_per_image, ground_truth)

        # è¿”å›ä¸¤ä¸ªæ–¹å‘æŸå¤±çš„å¹³å‡å€¼
        return (loss_i + loss_t) / 2


# è®­ç»ƒå‡½æ•°
def train(args, model, dataloader, device):
    """
    è®­ç»ƒè‡ªå®šä¹‰ CLIP å¯¹æ¯”å­¦ä¹ æ¨¡å‹ã€‚
    Args:
        args: è®­ç»ƒå‚æ•°
        model: åˆå§‹åŒ–å¥½çš„CLIPæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®­ç»ƒè®¾å¤‡
    """
    criterion = ContrastiveLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)

    print("Starting training...")
    for epoch in range(args.num_epochs):
        model.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.num_epochs}")
        for idx, batch in enumerate(pbar):
            input_ids = batch['input_ids'].to(device)
            pixel_values = batch['pixel_values'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            optimizer.zero_grad()

            image_features, text_features = model(pixel_values, input_ids, attention_mask)
            logit_scale = model.logit_scale

            loss = criterion(image_features, text_features, logit_scale)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            if (idx + 1) % args.log_steps == 0:
                pbar.set_postfix({'loss': f'{total_loss / (idx + 1):.4f}'})

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}")

    # ä¿å­˜æ¨¡å‹
    save_model_path = f"./models/CLIP/checkpoints/my_clip_{args.image_encoder_type}_epoch_{args.num_epochs}.pth"
    os.makedirs(os.path.dirname(save_model_path), exist_ok=True)
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™åˆ é™¤
    if os.path.exists(save_model_path):
        os.remove(save_model_path)
    torch.save(model.state_dict(), save_model_path)
    print(f"Model saved to {save_model_path}")

    return model

def evaluate(model, dataloader, device):
    """
    æ›´æ ‡å‡†ã€æ›´å…¨é¢åœ°è¯„ä¼° CLIP æ¨¡å‹åœ¨â€œä¸€å¯¹å¤šâ€æ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
    åˆ†åˆ«è®¡ç®— Image-to-Text å’Œ Text-to-Image çš„ Recall@1 å’Œ Recall@5ã€‚
    """
    model.eval()
    
    # åˆå§‹åŒ–å„ç§æŒ‡æ ‡çš„è®¡æ•°å™¨
    total_samples = 0
    i2t_r1_correct = 0
    i2t_r5_correct = 0
    t2i_r1_correct = 0
    t2i_r5_correct = 0

    print("ğŸš€ Starting comprehensive evaluation for retrieval...")
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            # pixel_values: [N, C, H, W]
            # input_ids: [5*N, max_len]
            pixel_values = batch['pixel_values'].to(device)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            num_images = pixel_values.shape[0]
            num_texts = input_ids.shape[0]

            # 1. è·å–ç‰¹å¾
            image_features, text_features = model(pixel_values, input_ids, attention_mask)
            logit_scale = model.logit_scale.exp()

            # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            # logits_per_image (I2T): [N, 5*N]
            logits_per_image = image_features @ text_features.T * logit_scale
            # logits_per_text (T2I): [5*N, N]
            logits_per_text = logits_per_image.T
            
            # --- 3. Image-to-Text (I2T) Recall è®¡ç®— ---
            # å¯¹äºç¬¬ i å¼ å›¾ç‰‡ï¼Œæ­£ç¡®çš„æ–‡æœ¬ç´¢å¼•æ˜¯ [i*5, i*5+1, ..., i*5+4]
            
            # I2T Recall@1
            # æ‰¾åˆ°æ¯å¼ å›¾ç‰‡æœ€åŒ¹é…çš„æ–‡æœ¬ç´¢å¼•
            i2t_preds_r1 = logits_per_image.argmax(dim=1)
            # æ£€æŸ¥é¢„æµ‹æ˜¯å¦åœ¨æ­£ç¡®èŒƒå›´å†…
            for i in range(num_images):
                if (i * 5) <= i2t_preds_r1[i] < ((i + 1) * 5):
                    i2t_r1_correct += 1

            # I2T Recall@5
            # æ‰¾åˆ°æ¯å¼ å›¾ç‰‡æœ€åŒ¹é…çš„å‰5ä¸ªæ–‡æœ¬ç´¢å¼•
            _, i2t_preds_r5_indices = logits_per_image.topk(5, dim=1)
            # æ£€æŸ¥è¿™top-5çš„é¢„æµ‹ä¸­ï¼Œæ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ªè½åœ¨æ­£ç¡®çš„5ä¸ªç­”æ¡ˆé‡Œ
            for i in range(num_images):
                pred_indices = set(i2t_preds_r5_indices[i].tolist())
                true_indices = set(range(i * 5, (i + 1) * 5))
                if len(pred_indices & true_indices) > 0:
                    i2t_r5_correct += 1

            # --- 4. Text-to-Image (T2I) Recall è®¡ç®— ---
            # å¯¹äºç¬¬ j ä¸ªæ–‡æœ¬ï¼Œæ­£ç¡®çš„å›¾ç‰‡ç´¢å¼•æ˜¯ floor(j / 5)
            ground_truth = torch.arange(num_texts, device=device) // 5 # [0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,...]

            # T2I Recall@1
            t2i_preds_r1 = logits_per_text.argmax(dim=1)
            t2i_r1_correct += (t2i_preds_r1 == ground_truth).sum().item()

            # T2I Recall@5
            _, t2i_preds_r5_indices = logits_per_text.topk(5, dim=1) # [N*5, 5]
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦å‡ºç°åœ¨ top-5 é¢„æµ‹ä¸­
            t2i_r5_correct += (t2i_preds_r5_indices == ground_truth.unsqueeze(1)).any(dim=1).sum().item()
            
            total_samples += num_images

    # --- 5. è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆç»“æœ ---
    i2t_r1 = 100 * i2t_r1_correct / total_samples
    i2t_r5 = 100 * i2t_r5_correct / total_samples
    # å¯¹äºT2Iï¼Œæ ·æœ¬æ€»æ•°æ˜¯ 5 * total_samples
    t2i_r1 = 100 * t2i_r1_correct / (total_samples * 5)
    t2i_r5 = 100 * t2i_r5_correct / (total_samples * 5)

    print("\nâœ… Evaluation Results:")
    print(f"  Image-to-Text Recall@1: {i2t_r1:.2f}%")
    print(f"  Image-to-Text Recall@5: {i2t_r5:.2f}%")
    print("-" * 30)
    print(f"  Text-to-Image Recall@1: {t2i_r1:.2f}%")
    print(f"  Text-to-Image Recall@5: {t2i_r5:.2f}%")
    
    # é€šå¸¸ä¼šæŠ¥å‘Šæ‰€æœ‰è¿™äº›æŒ‡æ ‡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„â€œå‡†ç¡®ç‡â€
    return {
        "i2t_r1": i2t_r1, "i2t_r5": i2t_r5,
        "t2i_r1": t2i_r1, "t2i_r5": t2i_r5
    }

# 6. ä¸»å‡½æ•°
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Custom CLIP Contrastive Model")

    # è·¯å¾„ç›¸å…³å‚æ•°
    parser.add_argument("--image_dir", type=str, default="./data/flickr30k_images/flickr30k_images")
    parser.add_argument("--text_data_path", type=str, default="./data/flickr30k_images/results.csv")
    # parser.add_argument("--save_model_path", type=str, default="./models/CLIP/checkpoints/my_clip_resnet_epoch_1.pth")

    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--num_epochs", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument("--log_steps", type=int, default=10)
    parser.add_argument("--eval", action="store_true", help="Run evaluation only")
    parser.add_argument("--train", action="store_true", help="Run training only") 

    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument("--model_name", type=str, default="openai/clip-vit-base-patch32")
    parser.add_argument("--pretrained_model_name", type=str, default="vit_base_patch16_clip_224.openai")
    parser.add_argument("--projection_dim", type=int, default=512)
    parser.add_argument("--max_seq_length", type=int, default=77)
    parser.add_argument("--image_encoder_type", type=str, default="vit-pretrained")
    # parser.add_argument("--temperature", type=float, default=0.07, help="Temperature parameter for contrastive loss") # logit_scale å·²ç»åŒ…å«

    # è§†è§‰ç¼–ç å™¨å‚æ•°
    parser.add_argument("--image_size", type=int, default=224)
    parser.add_argument("--patch_size", type=int, default=16)
    parser.add_argument("--vision_feature_dim", type=int, default=768)
    parser.add_argument("--resnet_feature_dim", type=int, default=512)
    parser.add_argument("--image_n_head", type=int, default=8)
    parser.add_argument("--image_n_layer", type=int, default=6)
    
    # æ–‡æœ¬ç¼–ç å™¨å‚æ•°
    parser.add_argument("--vocab_size", type=int, default=49408)
    parser.add_argument("--text_feature_dim", type=int, default=512)
    parser.add_argument("--text_n_head", type=int, default=8)
    parser.add_argument("--text_n_layer", type=int, default=6)

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # å‡†å¤‡æ•°æ®å’Œå¤„ç†å™¨
    processor = CLIPProcessor.from_pretrained(args.model_name)
    dataset = Flickr30kDataset(
        image_dir=args.image_dir,
        text_path=args.text_data_path,
        processor=processor,
        max_len=args.max_seq_length
    )
    print(dataset[0])
    print(dataset[0]['attention_mask'].shape)
    print(dataset[0]['input_ids'].shape)
    print(dataset[0]['pixel_values'].shape)
    
    # åˆ›å»ºè®­ç»ƒé›†å’Œè¯„ä¼°é›†
    total_size = len(dataset)
    train_size = total_size // 5  # è®­ç»ƒé›†å–1/5
    eval_size = total_size // 10  # è¯„ä¼°é›†å–1/10
    
    # éšæœºé‡‡æ ·ä¸é‡å çš„ç´¢å¼•
    all_indices = list(range(total_size))
    train_indices = random.sample(all_indices, train_size)
    remaining_indices = list(set(all_indices) - set(train_indices))
    eval_indices = random.sample(remaining_indices, eval_size)
    
    # åˆ›å»ºè®­ç»ƒé›†å’Œè¯„ä¼°é›†
    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    eval_dataset = torch.utils.data.Subset(dataset, eval_indices)

    print(f"Original dataset length: {total_size}")
    print(f"Training dataset length: {len(train_dataset)}")
    print(f"Evaluation dataset length: {len(eval_dataset)}")

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True,
        collate_fn=collate_fn
    )

    for i, batch in enumerate(train_dataloader):
        if i == 0:  
            print(batch['input_ids'].shape)
            print(batch['attention_mask'].shape)
            print(batch['pixel_values'].shape)
            # input("Press Enter to continue...")

    eval_dataloader = DataLoader(
        eval_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        collate_fn=collate_fn
    )

    # åˆå§‹åŒ–æ¨¡å‹
    image_encoder_vit = VisionEncoder(
        image_size=args.image_size,
        patch_size=args.patch_size,
        embed_dim=args.vision_feature_dim,
        n_head=args.image_n_head,
        n_layer=args.image_n_layer
    ).to(device)

    image_encoder_vit_pretrained = VisionEncoderPretrained(
        pretrained_model_name=args.pretrained_model_name,
        pretrained=True,
        embed_dim=args.vision_feature_dim
    ).to(device)

    image_encoder_resnet = ModifiedResNet(
        embed_dim=args.resnet_feature_dim,
        n_head=args.image_n_head
    ).to(device)

    text_encoder = TextEncoder(
        vocab_size=args.vocab_size,
        embed_dim=args.text_feature_dim,
        max_length=args.max_seq_length,
        n_head=args.text_n_head,
        n_layer=args.text_n_layer,
    ).to(device)

    # æ ¹æ®é€‰æ‹©çš„å›¾åƒç¼–ç å™¨ç±»å‹è®¾ç½®ç‰¹å¾ç»´åº¦
    if args.image_encoder_type == 'vit':
        image_encoder = image_encoder_vit
        vision_feature_dim = args.vision_feature_dim
    elif args.image_encoder_type == 'resnet':  # resnet
        image_encoder = image_encoder_resnet
        vision_feature_dim = args.resnet_feature_dim
    elif args.image_encoder_type == 'vit-pretrained':
        image_encoder = image_encoder_vit_pretrained
        vision_feature_dim = args.vision_feature_dim
    else:
        raise ValueError(f"Invalid image encoder type: {args.image_encoder_type}")

    model = CLIP(
        image_encoder=image_encoder,
        text_encoder=text_encoder,
        vision_feature_dim=vision_feature_dim,
        text_feature_dim=args.text_feature_dim,
        embed_dim=args.projection_dim,
    ).to(device)

    # ç»Ÿè®¡æ¨¡å‹å‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)
    print(f"\nModel Parameters:")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {non_trainable_params:,}")
    print(f"Total parameters: {trainable_params + non_trainable_params:,}\n")

    # å¦‚æœæ˜¯è¯„ä¼°æ¨¡å¼ï¼ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    save_model_path = f"./models/CLIP/checkpoints/my_clip_{args.image_encoder_type}_epoch_{args.num_epochs}.pth"
    if args.train:
        # è®­ç»ƒæ¨¡å¼
        train(args, model, train_dataloader, device)
    if args.eval:
        print(f"Loading model from {save_model_path}")
        model.load_state_dict(torch.load(save_model_path))
        evaluate(model, eval_dataloader, device)
    